{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227f1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db44b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a069c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('vocalization_df.feather')\n",
    "\n",
    "df_c2 = df[df.cohort=='c2']\n",
    "df_c4 = df[df.cohort=='c4']\n",
    "df_c5 = df[df.cohort=='c5']\n",
    "\n",
    "z = df.groupby('z_70')['timestamp'].count().values\n",
    "reorder = np.argsort(z)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_reorder = np.array([0,3,10,15,21,23,25,\n",
    "                           42,45,54,56,64,67])\n",
    "states = [np.where(reorder == states_reorder[i])[0][0] for i in range(len(states_reorder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "fs = 125000\n",
    "audio_basename = 'cohort2_combined_audio'\n",
    "\n",
    "for state in states:\n",
    "    print('Working on state {}'.format(state))\n",
    "    \n",
    "    #take longest duration examples for each state\n",
    "    df_sub = df_c2[df_c2.z_70 == state]\n",
    "    longest_dur_sort = np.argsort(df_sub.offset-df_sub.onset)[::-1].values\n",
    "    df_sub = df_sub.iloc[longest_dur_sort][:n_samples]\n",
    "    \n",
    "    #plot\n",
    "    figure(figsize=(10,10))\n",
    "    for i in range(n_samples):\n",
    "        working_onset, working_offset = df_sub.onset.values[i], df_sub.offset.values[i]\n",
    "        working_filename = df_sub.audio_filename.values[i]\n",
    "        \n",
    "        working_audio_slice = AudioSegment.from_file(os.path.join(audio_basename, working_filename), \n",
    "                                             codec='pcm_f64le', \n",
    "                                             start_second=working_onset, \n",
    "                                             duration=working_offset-working_onset)\n",
    "\n",
    "        working_audio = np.array(working_audio_slice.get_array_of_samples())\n",
    "    \n",
    "        v = np.zeros(int(.3*fs))\n",
    "        v[int((len(v)/2) - len(working_audio)/2):int((len(v)/2) +len(working_audio)/2)] = working_audio\n",
    "\n",
    "        subplot(10,10,i+1)\n",
    "        specgram(v, NFFT=512, noverlap=256, Fs=fs,clim=(-10,20), cmap='magma')\n",
    "        xticks([])\n",
    "        yticks([])\n",
    "        #axis('equal')\n",
    "    \n",
    "    savefig('multi_syllababic_examples_state_{}.png'.format(state), dpi=300)\n",
    "    close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

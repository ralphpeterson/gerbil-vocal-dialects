{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb5f415",
   "metadata": {},
   "source": [
    "# Install AVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e54dd",
   "metadata": {},
   "source": [
    "#### In your terminal:\n",
    "\n",
    ">git clone https://github.com/pearsonlab/autoencoded-vocal-analysis \\\n",
    ">cd autoencoded-vocal-analysis \\\n",
    ">pip install .\n",
    "\n",
    "Taken from https://github.com/pearsonlab/autoencoded-vocal-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fba7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#un-comment this and add where your ava repo was cloned if you get a 'no module found' error\n",
    "# import sys\n",
    "# ava_path = '/Users/rep359/code/autoencoded-vocal-analysis/' #path to ava directory\n",
    "# sys.path.append(ava_path) #this adds the location of ava to your path\n",
    "\n",
    "import os\n",
    "from ava.segmenting.segment import segment\n",
    "from ava.segmenting.amplitude_segmentation import get_onsets_offsets\n",
    "from ava.segmenting.utils import get_spec, softmax, clean_segments_by_hand\n",
    "from ava.segmenting.segment import tune_segmenting_params\n",
    "\n",
    "import vocalization_segmenting as vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81367bba",
   "metadata": {},
   "source": [
    "# Preprocessing Step 1: set initial segmentation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411b8de",
   "metadata": {},
   "source": [
    "For this step, you will need a single audio recording with vocalizations in it. A set of default parameters for segmentation have been chosen to start, but you will optimize these parameters for your particular dataset in step 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6043d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to an audio file of interest\n",
    "wav_path = '2020_07_22_15_52_33_369348_merged.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d8d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles, sr = vs.get_spec_min_max(wav_path, start_s=867.442, stop_s=867.442+15) #I picked a portion of the file where there were lots of vocalizations\n",
    "\n",
    "seg_params = {\n",
    "    'min_freq': 500, # minimum frequency\n",
    "    'max_freq': 62.5e3, # maximum frequency\n",
    "    'nperseg': 512, # FFT\n",
    "    'noverlap': 256, # FFT\n",
    "    'spec_min_val': -8, # minimum log-spectrogram value\n",
    "    'spec_max_val': -7.25, # maximum log-spectrogram value\n",
    "    'fs': 125000, # audio samplerate\n",
    "    'th_1':2, # segmenting threshold 1\n",
    "    'th_2':5, # segmenting threshold 2\n",
    "    'th_3':2, # segmenting threshold 3\n",
    "    'min_dur':0.03, # minimum syllable duration\n",
    "    'max_dur': 0.3, # maximum syllable duration\n",
    "    'smoothing_timescale': 0.007, # amplitude\n",
    "    'softmax': False, # apply softmax to the frequency bins to calculate\n",
    "                      # amplitude\n",
    "    'temperature':0.5, # softmax temperature parameter\n",
    "    'algorithm': get_onsets_offsets, # (defined above)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa0d341",
   "metadata": {},
   "source": [
    "# Preprocessing Step 2: tune parameters + segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53c85a",
   "metadata": {},
   "source": [
    "Navigate to your audio directory and look at 'tuning.pdf' after clicking through the initial parameter tuning cycle below (no need to change anything the firs time around). Adjust your 'th_1' and 'th_3' to a threshold value that makes sense given the vocalization amplitude traces. I usually keep them the same - more info [here](https://autoencoded-vocal-analysis.readthedocs.io/en/latest/segment.html). Re-run the tuning cycle. Do the onsets (blue) and offsets (red) look correct? If so, move on. If not, continue tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00192c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune segmenting parameters\n",
      "---------------------------\n",
      "Set value for min_freq: [500] \n",
      "Set value for max_freq: [62500.0] \n",
      "Set value for nperseg: [512] \n",
      "Set value for noverlap: [256] \n",
      "Set value for spec_min_val: [-8] \n",
      "Set value for spec_max_val: [-7.25] \n",
      "Set value for fs: [125000] \n",
      "Set value for th_1: [2] \n",
      "Set value for th_2: [5] \n",
      "Set value for th_3: [2] \n",
      "Set value for min_dur: [0.03] \n",
      "Set value for max_dur: [0.3] \n",
      "Set value for smoothing_timescale: [0.007] \n",
      "Set value for temperature: [0.5] \n",
      "searching\n",
      "searching\n",
      "searching\n",
      "Continue? [y] or [s]top tuning or [r]etune params: s\n"
     ]
    }
   ],
   "source": [
    "audio_directories = [os.path.dirname(wav_path)] # list of audio directories\n",
    "tuning_fn = os.path.join(os.path.dirname(wav_path), 'tuning.pdf')\n",
    "seg_params = tune_segmenting_params(audio_directories, seg_params, img_fn='seg-test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd662ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(os.path.dirname(wav_path), os.path.join(os.path.dirname(wav_path), 'segment'), seg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4d33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae0dbc15",
   "metadata": {},
   "source": [
    "# Processing (parallel): get onsets/offsets for many files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70176272",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dirs = ['cohort2_combined_audio',\n",
    "              'cohort4_combined_audio',\n",
    "              'cohort5_combined_audio']\n",
    "\n",
    "segment_dirs = [os.path.join(audio_dir, 'segments') for audio_dir in audio_dirs]\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import repeat\n",
    "\n",
    "gen = zip(audio_dirs, segment_dirs, repeat(seg_params))\n",
    "Parallel(n_jobs=-1, verbose=11)(delayed(segment)(*args) for args in gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba22117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f07ab22",
   "metadata": {},
   "source": [
    "# Vocalization extraction: compute spectral flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96522300",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob('*/*.wav') #get paths to all the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through all your files and filter out vox from non vox using spectral flatness\n",
    "for fn in fns:\n",
    "    vs.filter_segments(fn)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
